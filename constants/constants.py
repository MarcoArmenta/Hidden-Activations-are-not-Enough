ARCHITECTURES = [(500, 500, 500, 500, 500),

                 (1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000),

                 (1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,
                  1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000),

                 (10000, 10000),

                 (10000, 10000, 10000, 10000, 10000),

                 (10000, 9000, 8000, 7000, 6000, 5000, 4000, 3000, 2000, 1000),
               ]

# TODO: Train BIG mlp for a lot of time
# TODO: pretrained mlps on the internet

ATTACKS = ["GN", "FGSM", "RFGSM", "PGD", "EOTPGD", "FFGSM", "TPGD", "MIFGSM", "UPGD", "DIFGSM", "NIFGSM",
           "PGDRS", "SINIFGSM", "VMIFGSM", "VNIFGSM", "CW", "PGDL2", "PGDRSL2", "DeepFool", "SparseFool",
           "OnePixel", "Pixle", "FAB"]

DEFAULT_EXPERIMENTS = {
    'experiment_0': {
        'architecture_index': 0,
        'dataset': 'mnist',
        'optimizer': 'sgd',
        'lr': 0.01,
        'batch_size': 8,
        'epoch': 1,
        'reduce_lr_each': 5,
        'save_every_epochs': 2,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_1': {
        'architecture_index': 0,
        'optimizer': 'momentum',
        'dataset': 'mnist',
        'lr': 0.01,
        'batch_size': 32,
        'epoch': 11,
        'reduce_lr_each': 5,
        'save_every_epochs': 2,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_2': {
        'architecture_index': 0,
        'optimizer': 'adam',
        'dataset': 'fashion',
        'lr': 1e-06,
        'batch_size': 32,
        'epoch': 81,
        'reduce_lr_each': 20,
        'save_every_epochs': 10,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_3': {
        'architecture_index': 0,
        'optimizer': 'sgd',
        'dataset': 'fashion',
        'lr': 0.1,
        'batch_size': 16,
        'epoch': 51,
        'reduce_lr_each': 20,
        'save_every_epochs': 10,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_4': {
        'architecture_index': 1,
        'optimizer': 'momentum',
        'dataset': 'mnist',
        'lr': 0.01,
        'batch_size': 32,
        'epoch': 21,
        'reduce_lr_each': 20,
        'save_every_epochs': 5,
        'residual': True,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_5': {
        'architecture_index': 2,
        'optimizer': 'adam',
        'dataset': 'fashion',
        'lr': 0.001,
        'batch_size': 16,
        'epoch': 11,
        'reduce_lr_each': 5,
        'save_every_epochs': 5,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_6': {
        'architecture_index': 3,
        'optimizer': 'momentum',
        'dataset': 'mnist',
        'lr': 0.01,
        'batch_size': 32,
        'epoch': 1,
        'reduce_lr_each': 5,
        'save_every_epochs': 1,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_7': {
        'architecture_index': 3,
        'optimizer': 'momentum',
        'dataset': 'fashion',
        'lr': 0.01,
        'batch_size': 32,
        'epoch': 11,
        'reduce_lr_each': 5,
        'save_every_epochs': 1,
        'residual': False,
        'weight_decay': 0,
        'dropout': False,
    },
    'experiment_8': { #TODO: check performance 99 train vs 60 test, uses weight decay and drop out, the others dont
        'architecture_index': 3,
        'optimizer': 'momentum',
        'dataset': 'cifar10',
        'lr': 0.01,
        'batch_size': 64,
        'epoch': 101,
        'reduce_lr_each': 20,
        'save_every_epochs': 1,
        'residual': False,
        'weight_decay': 1e-5,
        'dropout': True,
    },
    'experiment_9': { #TODO: check performance
        'architecture_index': 4,
        'optimizer': 'adam',
        'dataset': 'cifar10',
        'lr': 0.001,
        'batch_size': 256,
        'epoch': 201,
        'reduce_lr_each': 40,
        'save_every_epochs': 50,
        'residual': False,
        'weight_decay': 1e-5,
        'dropout': True,
    },
    'experiment_10': { #TODO: check performance
        'architecture_index': 5,
        'optimizer': 'adam',
        'dataset': 'cifar10',
        'lr': 0.001,
        'batch_size': 256,
        'epoch': 201,
        'reduce_lr_each': 40,
        'save_every_epochs': 50,
        'residual': False,
        'weight_decay': 1e-5,
        'dropout': True,
    }
}
